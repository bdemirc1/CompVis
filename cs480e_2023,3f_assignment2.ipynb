{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdemirc1/CompVis/blob/main/cs480e_2023%2C3f_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "**Due November 12th, 11:59 PM**"
      ],
      "metadata": {
        "id": "tUheapJCOaxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Busra Demirci<br>\n",
        "B-Number: B00840451<br>\n",
        "Email: bdemirc1@binghamton.edu"
      ],
      "metadata": {
        "id": "26iemMN3qq0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following assignment,\n",
        "you will be implementing functions and their analytical derivatives to train linear classifiers and neural networks on the MNIST dataset.\n",
        "You are allowed and expected to use NumPy.\n",
        "You are not allowed to use PyTorch.\n",
        "\n",
        "Tasks that need to be completed are indicated with a\n",
        "right-pointing triangle (&#9658;)\n",
        "or clearly stated in the experiments section.\n",
        "\n",
        "<!--\n",
        "The experiments section for each classifier also need to be implemented. You should follow the instructions above the cell. You may also add additional cells.\n",
        "-->\n",
        "\n",
        "Cells that need to be run to set up the appropriate infrastructure are indicated with a downward-pointing triangle (&#9660;).\n",
        "Such cells do not need to be modified.\n",
        "Make sure you have run the previous cells before running the current cell, or you may get an error.\n",
        "\n",
        "Submission will be via GitHub Classroom. **You are required to have at least 10 commits for this assignment.**"
      ],
      "metadata": {
        "id": "jqLULe5pxwpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import statements\n",
        "\n",
        "&#9660;Run the cell below to import the packages needed for the code below.\n",
        "Most other packages are also okay,\n",
        "but you must ask first."
      ],
      "metadata": {
        "id": "Hnw_h7A1Orc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JLEavoS9O9g-"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Backpropagation"
      ],
      "metadata": {
        "id": "VIBNL5SMPMt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Linear Transforms\n",
        "\n",
        "A linear classifier computes a vector of scores for a single sample,\n",
        "with one score for each class.\n",
        "Let $D$ be the number of features and\n",
        "$M$ be the number of classes.\n",
        "Then the score $y_{i,k}$ for the $k$-th class of the $i$-th sample\n",
        "is computed by:\n",
        "\n",
        "$$\n",
        "y_{i,k} = \\sum_{j = 1}^{D} w_{j,k}x_{i,j} + b_k\n",
        "$$\n",
        "\n",
        "where $w_{j,k}$ is the $j$-th weight for the $k$-th class,\n",
        "$x_{i,j}$ is the $j$-th feature for the $i$-th sample,\n",
        "and $b_k$ is the bias term for class $k$.\n",
        "\n",
        "During training,\n",
        "we often group $N$ samples into what is called a *minibatch*,\n",
        "and process the whole minibatch at once.\n",
        "This is more efficient,\n",
        "and also improves the gradient descent convergence.\n",
        "Letting each sample be a row in the $N\\times D$ matrix $X$,\n",
        "we can then write this as\n",
        "\n",
        "$$\n",
        "Y = XW + B\n",
        "$$\n",
        "\n",
        "where $W$ is the $D \\times M$ weight matrix.\n",
        "The weights for each class form a column in $W$.\n",
        "All the biases have been collected into a single matrix $B$.\n",
        "\n",
        "Although the above can work,\n",
        "we can turn it into a single matrix multiplication\n",
        "via the &ldquo;bias trick&rdquo;,\n",
        "which adds an extra dummy feature in the input sample\n",
        "that is always hard-coded to 1,\n",
        "and then adding an extra weight that is the bias term.\n",
        "\n",
        "$$\n",
        "y_{i,k} = \\sum_{j = 1}^{D + 1} w_{j,k}x_{i,j}, \\ \\ \\text{with}\\ \\ x_{i,D+1} = 1\n",
        "$$\n",
        "\n",
        "The scores for a whole minibatch can now be computed via a standard matrix\n",
        "multiplication.\n",
        "\n",
        "$$\n",
        "Y = X'W'\n",
        "$$\n",
        "\n",
        "where $X'$ and $W'$ are the augmented sample and weight matrices.\n",
        "This transformation forms the basis of a linear, fully-connected (also called &ldquo;dense&rdquo;) layer in a neural network."
      ],
      "metadata": {
        "id": "VhFZKNmUXzt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9654; Implement `linear_forward(X, W)` in the cell below to perform the forward pass of\n",
        "a single linear layer on a batch of samples $X$,\n",
        "using the weights from $W$.\n",
        "The matrix $W$ has already had the bias added to it,\n",
        "so $X$ will need to be augmented with the hard-coded 1."
      ],
      "metadata": {
        "id": "KszunLwwRJqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(X, W):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a linear transformation.\n",
        "\n",
        "    Consider a linear layer that accepts inputs with D features,\n",
        "    and has M neurons.  Assume that our minibatch size\n",
        "    is N.  In other words, we wish to process N samples at once.\n",
        "\n",
        "    The input X has shape (N, D) and contains a minibatch of N\n",
        "    samples, where each sample X[i] has shape (D).  Each sample\n",
        "    will be transformed to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array containing input data, of shape (N, D)\n",
        "    - W: A numpy array of weights, of shape (D+1, M)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (X, W)\n",
        "\n",
        "    The returned (X, W) is redundant, but makes the training code\n",
        "    more concise.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    out = None # Initialize the out variable.\n",
        "\n",
        "    #\n",
        "    # PUT YOUR CODE BELOW: Below, implement the linear forward pass. Store the result in out.\n",
        "    # Make sure to do the bias trick!\n",
        "    #\n",
        "    N, D = X.shape\n",
        "    #print(N, D)\n",
        "    bias = np.ones((N, 1))\n",
        "    X_= np.hstack((X, bias))\n",
        "    #print(X_)\n",
        "    out = np.matmul(X_, W)\n",
        "    #print(out)\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    cache = (X, W)\n",
        "\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "fsl6QHm_PJdi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "W_test = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]])\n",
        "\n",
        "# Call the linear_forward function.\n",
        "out_test, cache_test = linear_forward(X_test, W_test)\n",
        "print(out_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC5GVwKp5rmm",
        "outputId": "a709d935-c18f-4081-d39e-993c832abbfa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.9 3.6]\n",
            " [5.6 7.2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement `linear_backward(d_upstream, cache)` that returns the downstream analytical gradients with respect to $X$ and $W$,\n",
        "given the upstream gradients and $X$ and $W$.\n",
        "See here for details on how to [backpropagate through a linear layer.](https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html)"
      ],
      "metadata": {
        "id": "VLnk9xwpRf2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(d_upstream, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an linear layer.\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - X: Input data, of shape (N, D)\n",
        "      - W: Weights, of shape (D+1, M)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dX: Gradient of the output of this layer with respect to X, of shape (N, D).\n",
        "          This is the downstream gradient.\n",
        "    - dW: Gradient with respect to W, of shape (D+1, M)\n",
        "    \"\"\"\n",
        "    X, W = cache\n",
        "    dX, dW = None, None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the linear backward pass by calculating the\n",
        "    # gradient with respect to the cached inputs X and W. Store them in the\n",
        "    # variables dX and dW.\n",
        "\n",
        "    # dL/dX = dL/dY . Wt\n",
        "    # dL/dW = Xt . dL/dY\n",
        "    #\n",
        "    N, D = X.shape\n",
        "    X_ = np.hstack((X, np.ones((N, 1))))\n",
        "\n",
        "    W_transposed = np.transpose(W)\n",
        "    X_transposed = np.transpose(X_)\n",
        "    dX = np.matmul(d_upstream, W_transposed)\n",
        "    dW = np.matmul(X_transposed, d_upstream)\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "    return dX, dW"
      ],
      "metadata": {
        "id": "41ac6dd5TSEX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_upstream_test = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "X_test = np.array([[1.0, 2.0, 1.0], [3.0, 4.0, 1.0]])\n",
        "W_test = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]])\n",
        "\n",
        "dX_test, dW_test = linear_backward(d_upstream_test, (X_test, W_test))\n",
        "\n",
        "print(\"Gradient with respect to X (dX_test):\")\n",
        "print(dX_test)\n",
        "print(\"Gradient with respect to W (dW_test):\")\n",
        "print(dW_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIwctkoUenQU",
        "outputId": "1c24e01e-6683-4f2b-fac9-dfa1d68971b6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient with respect to X (dX_test):\n",
            "[[0.5 1.1 1.7 2.3]\n",
            " [1.1 2.5 3.9 5.3]]\n",
            "Gradient with respect to W (dW_test):\n",
            "[[10. 14.]\n",
            " [14. 20.]\n",
            " [ 4.  6.]\n",
            " [ 4.  6.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Checking Gradients with Finite Differences\n",
        "\n",
        "Numerical code can be difficult to debug.\n",
        "The general approach is to compare the answer given\n",
        "by your code to the answer obtained from some other technique.\n",
        "A finite difference is a numerical approximation to the derivative which can be used to check your gradients.\n",
        "Because it is only an approximation,\n",
        "you do not use it for actual training,\n",
        "however.\n",
        "\n",
        "The multi-variate central finite difference for a function $f(x,y)$ is given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial  f}{\\partial x} = \\frac{f(x+h, y)-f(x-h, y)}{2h}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\frac{\\partial  f}{\\partial y} = \\frac{f(x, y+h)-f(x, y-h)}{2h}\n",
        "$$\n",
        "\n",
        "The above pattern holds for functions with higher number of variables.\n",
        "For our purposes,\n",
        "an $h$ of about $10^{-9}$ should be adequate."
      ],
      "metadata": {
        "id": "lk1XRA9LSKml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; In the next cell,\n",
        "implement the `finite_difference_linear(d_upstream, cache, h)` function.\n",
        "This function is analogous to `linear_backward()`\n",
        "in that it computes the derivative matrices\n",
        "$\\frac{\\partial L}{\\partial X}$ and\n",
        "$\\frac{\\partial L}{\\partial W}$,\n",
        "given an upstream gradient,\n",
        "but it actually estimates the local gradient\n",
        "using a finite difference.\n",
        "The `h` parameter corresponds to $h$ above;\n",
        "the other parameters are the same as for `linear_backward()`.\n",
        "\n",
        "Recall that the downstream gradient can be computed from\n",
        "the local gradient and upstream gradient by applying the\n",
        "chain rule.\n",
        "In this case,\n",
        "we need the multivariable chain rule,\n",
        "because the loss is a function of the matrix $Y$,\n",
        "and $Y$ is a function of the matrices $X$ and $W$.\n",
        "(Any non-singleton matrix is multivariable by definition.)\n",
        "In particular,\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_{i,j}}\n",
        "= \\sum^{N}_{k=1}\\sum^{D + 1}_{l=1} \\frac{\\partial L}{\\partial y_{k,l}}\n",
        "\\frac{\\partial y_{k,l}}{\\partial x_{i,j}}\n",
        "$$\n",
        "\n",
        "where the $\\frac{\\partial L}{\\partial y_{k,l}}$ form the upstream\n",
        "gradient and the $\\frac{\\partial y_{k,l}}{\\partial x_{i,j}}$ form the local gradient.\n",
        "(Note that $\\frac{\\partial y_{k,l}}{\\partial x_{i,j}} = 0$ in our case when $k \\neq i$,\n",
        "because a given sample does not affect the output for other samples\n",
        "in the same minibatch.\n",
        "This fact could be used to optimize the code,\n",
        "but there is no requirement to do so for this assignment.)\n",
        "This simplifies nicely as the gradient for each variable in the matrix is the sum of the products of each upstream partial derivative\n",
        "$\\frac{\\partial L}{\\partial y_{k,l}}$\n",
        "from `d_upstream` and the corresponding element in the finite difference matrix.\n",
        "(This operation is analogous to the dot product,\n",
        "except it is performed on matrices instead of vectors,\n",
        "and is called the\n",
        "[Frobenius product](https://en.wikipedia.org/wiki/Frobenius_inner_product).)"
      ],
      "metadata": {
        "id": "Je8LC4OnT-rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finite_difference_linear(d_upstream, cache, h):\n",
        "    '''\n",
        "    Computes the numerical gradient for a linear layer\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - X: Input data, of shape (N, D)\n",
        "      - W: Weights, of shape (D+1, M)\n",
        "    - h: The h to use in the finite difference.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dX: Gradient with respect to X, of shape (N, D).  This is the downstream\n",
        "          gradient.\n",
        "    - dW: Gradient with respect to W, of shape (D+1, M)\n",
        "    '''\n",
        "\n",
        "    dX = None\n",
        "    dW = None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the finite difference for the linear\n",
        "    # function.  Return the gradient at input (X,W) w.r.t to x and w.\n",
        "\n",
        "    X, W = cache\n",
        "    N, D = X.shape\n",
        "    D1, M = W.shape\n",
        "\n",
        "    dX = np.zeros((N, D1))\n",
        "    dW = np.zeros((D1, M))\n",
        "\n",
        "    X_ = np.hstack((X, np.ones((N, 1))))\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(D1):\n",
        "            Xph = X_.copy()\n",
        "            Xmh = X_.copy()\n",
        "            Xph[i, j] += h\n",
        "            Xmh[i, j] -= h\n",
        "\n",
        "            Rph = np.dot(Xph, W)\n",
        "            Rmh= np.dot(Xmh, W)\n",
        "\n",
        "            dX[i, j] = (np.sum(d_upstream * (Rph - Rmh)) / (2 * h))\n",
        "\n",
        "    for i in range(D1):\n",
        "        for j in range(M):\n",
        "            Wph= W.copy()\n",
        "            Wmh = W.copy()\n",
        "            Wph[i, j] += h\n",
        "            Wmh[i, j] -= h\n",
        "\n",
        "            Rph = np.dot(X_, Wph)\n",
        "            Rmh = np.dot(X_, Wmh)\n",
        "\n",
        "            dW[i, j] = (np.sum(d_upstream * (Rph- Rmh)) / (2 * h))\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    return dX, dW"
      ],
      "metadata": {
        "id": "HvUDtyAWUx7u"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_upstream_test = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "X_test = np.array([[1.0, 2.0, 1.0], [3.0, 4.0, 1.0]])\n",
        "W_test = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]])\n",
        "h_test = 1e-9\n",
        "\n",
        "dX_fd_test, dW_fd_test = finite_difference_linear(d_upstream_test, (X_test, W_test), h_test)\n",
        "print(dX_fd_test)\n",
        "print(dW_fd_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVgTS_e4rfd4",
        "outputId": "4dd954da-d600-433c-e6ea-51cd6c35d657"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.50000004 1.10000009 1.6999997  2.30000019]\n",
            " [1.10000009 2.50000021 3.90000032 5.29999977]]\n",
            "[[10.00000072 13.99999938]\n",
            " [14.00000094 20.00000165]\n",
            " [ 4.00000022  5.99999828]\n",
            " [ 3.99999966  5.99999916]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run this cell to do a gradient check to test the analytical gradients from the `linear_backward()` function with `finite_difference_linear()`."
      ],
      "metadata": {
        "id": "iTNHRnDDvCMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_check_linear():\n",
        "    N = 16\n",
        "    D = 4\n",
        "    C = 3\n",
        "\n",
        "    test_weight = np.random.random((D+1, C))\n",
        "    test_input = np.random.random((N, D))\n",
        "    dout = np.random.random((N, C))\n",
        "\n",
        "    cache = (test_input, test_weight)\n",
        "\n",
        "    grad_x_numerical, grad_w_numerical = finite_difference_linear(dout, cache, 1E-9)\n",
        "    print(\"finite dx:\")\n",
        "    print(grad_x_numerical)\n",
        "    print(\"finite dw:\")\n",
        "    print(grad_w_numerical)\n",
        "    grad_x_analytical, grad_w_analytical = linear_backward(dout, cache)\n",
        "    print(\"linear dx:\")\n",
        "    print(grad_x_analytical)\n",
        "    print(\"linear dw:\")\n",
        "    print(grad_w_analytical)\n",
        "\n",
        "    check_input_gradient = np.allclose(grad_x_numerical, grad_x_analytical)\n",
        "    check_weight_gradient = np.allclose(grad_w_numerical, grad_w_analytical)\n",
        "\n",
        "    if not check_input_gradient:\n",
        "        print(\"The gradient with respect to x failed\")\n",
        "\n",
        "    if not check_weight_gradient:\n",
        "        print(\"The gradient respect to w failed\")\n",
        "    print()\n",
        "    print(\"gradient check for linear passed!\")\n",
        "\n",
        "gradient_check_linear()"
      ],
      "metadata": {
        "id": "jvmbfkfRPSZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39cfb58-bb2c-4ec6-9dde-74b92af12ee8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finite dx:\n",
            "[[0.76441136 0.54052466 0.49823139 0.7890139  0.63988831]\n",
            " [0.73953599 0.9977189  0.85116798 1.0072826  0.66150581]\n",
            " [0.48434997 1.05072023 0.9112931  0.84745501 0.38355104]\n",
            " [0.53551831 0.72880247 0.69755325 0.70486202 0.33496674]\n",
            " [0.38402396 0.7789337  0.68691633 0.64173535 0.28697391]\n",
            " [0.56591338 0.40309834 0.3944876  0.57712695 0.42971522]\n",
            " [0.2645346  0.45989865 0.40855827 0.4044922  0.20033076]\n",
            " [0.48786144 0.65835286 0.58809294 0.65494014 0.38624567]\n",
            " [0.74468485 0.79005154 0.73324511 0.88835551 0.56537724]\n",
            " [0.37037019 0.54992124 0.46514224 0.53035422 0.33617052]\n",
            " [0.43033164 0.25417812 0.29651498 0.3983968  0.25148744]\n",
            " [0.31243411 0.75136042 0.63169137 0.58933252 0.2793161 ]\n",
            " [0.78678151 1.14922644 1.01165695 1.10293161 0.63981417]\n",
            " [0.25776759 0.80627541 0.67766873 0.57775671 0.22512935]\n",
            " [0.22602701 0.53029529 0.47564317 0.40897507 0.14596177]\n",
            " [0.6679991  0.46805317 0.40545809 0.69697858 0.60921176]]\n",
            "finite dw:\n",
            "[[3.22822276 3.85928481 3.94308546]\n",
            " [3.41340892 3.82345284 3.7526363 ]\n",
            " [3.00221062 2.77941401 3.60769797]\n",
            " [4.55222742 4.12140699 3.79356252]\n",
            " [7.66447833 7.33070293 7.7870237 ]]\n",
            "linear dx:\n",
            "[[0.76441161 0.54052449 0.49823157 0.78901389 0.63988822]\n",
            " [0.73953599 0.99771885 0.85116793 1.00728272 0.66150583]\n",
            " [0.48434984 1.05072032 0.91129317 0.84745491 0.38355096]\n",
            " [0.5355183  0.72880259 0.69755324 0.7048619  0.33496671]\n",
            " [0.38402394 0.77893372 0.68691631 0.64173539 0.28697394]\n",
            " [0.56591335 0.40309833 0.39448761 0.57712695 0.42971524]\n",
            " [0.26453463 0.45989861 0.40855827 0.40449218 0.20033077]\n",
            " [0.48786139 0.65835288 0.58809294 0.65494015 0.38624564]\n",
            " [0.74468472 0.79005165 0.73324501 0.88835552 0.56537723]\n",
            " [0.37037023 0.54992121 0.4651423  0.5303542  0.33617049]\n",
            " [0.43033162 0.2541781  0.29651499 0.39839681 0.25148742]\n",
            " [0.31243418 0.75136031 0.63169148 0.58933256 0.27931603]\n",
            " [0.78678164 1.14922626 1.01165705 1.10293148 0.63981421]\n",
            " [0.25776763 0.80627533 0.67766881 0.57775681 0.22512928]\n",
            " [0.22602701 0.53029526 0.4756432  0.40897507 0.14596179]\n",
            " [0.66799914 0.46805312 0.40545815 0.69697864 0.60921177]]\n",
            "linear dw:\n",
            "[[3.2282228  3.85928455 3.94308528]\n",
            " [3.41340886 3.82345289 3.75263632]\n",
            " [3.00221054 2.7794139  3.60769767]\n",
            " [4.55222719 4.1214067  3.79356272]\n",
            " [7.66447822 7.33070266 7.78702382]]\n",
            "\n",
            "gradient check for linear passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Linear Classifiers\n",
        "\n",
        "In this section,\n",
        "we will build upon the previously implemented layer to\n",
        "create linear classifiers.\n",
        "We will train them on high-dimensional real world data."
      ],
      "metadata": {
        "id": "i1DiweHpU-_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 MNIST Dataset\n",
        "\n",
        "The dataset we will use is MNIST,\n",
        "a set of images of handwritten digits compiled by the National Institute of Standards and Technology (NIST).\n",
        "This dataset is widely used as a an example for machine learning algorithms for image classification.\n",
        "The images are 28x28 pixels with a single grayscale channel ranging from 0 to 255.\n",
        "\n",
        "Data sets are typically split into\n",
        "[training, validation, and test sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets).\n",
        "The training set is used to adjust the weights in the model via\n",
        "gradient descent.\n",
        "The validation set is used during training to evaluate\n",
        "the progress of the training,\n",
        "and thus to\n",
        "[select the best model](https://en.wikipedia.org/wiki/Model_selection).\n",
        "The gradient is not computed when using the validation set,\n",
        "thus it does not take part in the gradient descent.\n",
        "The test set is used to evaluate\n",
        "the final results after training.\n",
        "The distinction between the\n",
        "validation and test set is that the validation set\n",
        "can be used to adjust hyperparameters (such as learning rate and model size),\n",
        "and to prevent overfitting, etc.,\n",
        "but the test set cannot be used except to evaluate the final results.\n",
        "\n",
        "You will be using 20,000 samples from the original training dataset for our next set of experiments,\n",
        "and the 10,000 sample test set."
      ],
      "metadata": {
        "id": "38HrO_eBXjrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following cell to define some helper functions for loading the MNIST data."
      ],
      "metadata": {
        "id": "dU3af5u3vY2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads and parses CSV file.\n",
        "# Returns a 2-D array containing the images, and a 1-D array\n",
        "# with the labels.  In the image array,\n",
        "# each row is an image.  Pixel values are from 0 to 255.\n",
        "def mnist_data_parser_helper(csv_file_name):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(csv_file_name,'r') as _file:\n",
        "        csv_reader = csv.reader(_file, delimiter=\",\")\n",
        "        for row in csv_reader:\n",
        "            Y.append(float(row[0]))\n",
        "            X.append([float(i) for i in row[1:]])\n",
        "    return (np.array(X), np.array(Y))\n",
        "\n",
        "def get_mnist_train_data():\n",
        "    X_train, Y_train = mnist_data_parser_helper(\"sample_data/mnist_train_small.csv\")\n",
        "    return X_train, Y_train\n",
        "\n",
        "def get_mnist_test_data():\n",
        "    X_test, Y_test = mnist_data_parser_helper(\"sample_data/mnist_test.csv\")\n",
        "    return X_test, Y_test"
      ],
      "metadata": {
        "id": "y6p9vHGhfBzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following cell to visualize some samples from the MNIST dataset."
      ],
      "metadata": {
        "id": "nsEncnA3vh9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_mnist_train_data()\n",
        "\n",
        "# Visualize some examples from the dataset.\n",
        "# We show a few examples of training images from each class.\n",
        "classes = list(range(10))\n",
        "\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train.astype('uint8') == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(x_train[idx].astype('uint8').reshape(28,28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "21II-zCpe-ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following cell to gather the training and testing dataset.\n"
      ],
      "metadata": {
        "id": "SLbwp3V-vxg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_mnist_train_data()  # Get the training dataset\n",
        "x_test, y_test = get_mnist_test_data()     # Get the test dataset"
      ],
      "metadata": {
        "id": "ZY8RrHx4Fegk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following to normalize the input dataset to have a mean of 0 and standard deviation of 1."
      ],
      "metadata": {
        "id": "d4cTK2RDv5wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std()\n",
        "\n",
        "x_train = (x_train - x_mean)/(x_std)\n",
        "x_test = (x_test - x_mean)/(x_std)"
      ],
      "metadata": {
        "id": "n7RKsVNmfTx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following cell to check the dimensions of the data."
      ],
      "metadata": {
        "id": "LfRlRMS9PTZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, dim = x_train.shape\n",
        "N_test, _ = x_test.shape\n",
        "print(f\"Number of training sample {N} with {dim} pixels per image\")\n",
        "print(f\"Number of training sample {N_test} with {dim} pixels per image\")"
      ],
      "metadata": {
        "id": "59jB-sjwQNVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Put code in the following cell to split the `x_train` and `y_train` arrays to training and validations sets with an 80-20 split ratio. Place the split arrays in to the `DATA` dictionary. This dictionary will be used to feed data into the `Solver`."
      ],
      "metadata": {
        "id": "QEggmFxlkHYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform training and validation dataset splits\n",
        "\n",
        "# PUT YOUR CODE BELOW\n",
        "DATA = {\"X_train\": None,      # Replace with the value here\n",
        "        \"X_val\" : None,       # Replace with the value here\n",
        "        \"y_train\" : None,     # Replace with the value here\n",
        "        \"y_val\" : None}       # Replace with the value here\n"
      ],
      "metadata": {
        "id": "-ZUHh5UJkHYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Implement and Evaluate ML Models\n",
        "\n",
        "We will follow a common organization for our code implementation.\n",
        "The `Solver` class is used to orchestrate the\n",
        "overall training steps,\n",
        "such as loading the data,\n",
        "doing the forward pass,\n",
        "computing the gradients with\n",
        "the backward pass,\n",
        "and updating the weights.\n",
        "The `Solver` class\n",
        "uses a *model*\n",
        "object which contains the actual weights,\n",
        "and implements the forward and backward passes."
      ],
      "metadata": {
        "id": "TDV7xaD_BkEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following cell to define the stochastic gradient descent algorithm that we will use to optimize our models."
      ],
      "metadata": {
        "id": "xXdYUcfTa78v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(w, dw, lr=1e-2):\n",
        "    \"\"\"\n",
        "    Performs vanilla stochastic gradient descent.\n",
        "\n",
        "    config format:\n",
        "    - learning_rate: Scalar learning rate.\n",
        "    \"\"\"\n",
        "\n",
        "    w -= lr * dw\n",
        "    return w"
      ],
      "metadata": {
        "id": "elsI_qFOa8HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following cell to define the `Solver` class."
      ],
      "metadata": {
        "id": "DyQdB2T4r9No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(object):\n",
        "    \"\"\"\n",
        "    Solver class for the learnable models using\n",
        "    mini-batch gradient descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 data,\n",
        "                 learning_rate=1e-3,\n",
        "                 num_epochs=50,\n",
        "                 batch_size=200,\n",
        "                 validation_frequency=16):\n",
        "        \"\"\"\n",
        "        Construct a new Solver instance.\n",
        "\n",
        "        Inputs:\n",
        "          model: Python class equiped with forward, backward, predict methods and a params dictionary\n",
        "          data: Dictionary with X_train, X_val,  Y_train, Y_val keys\n",
        "          learning_rate: Float, step size of the optimizer\n",
        "          num_epochs: Int, Number of times to completely traverse X_train\n",
        "          batch_size: Int, The number of samples in update\n",
        "          validation_frequency: Int, Solver performs validation loop every validation_frequency batches.\n",
        "                               Set this to a high number if num_epochs is large\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.validation_frequency = validation_frequency\n",
        "\n",
        "        self.num_training = data[\"X_train\"].shape[0]\n",
        "        self.input_dim = data[\"X_train\"].shape[1]\n",
        "\n",
        "        intervals = list(range(0, self.num_training, batch_size))[1:]\n",
        "\n",
        "        self.X_train = np.array_split(data[\"X_train\"], intervals, axis=0)\n",
        "        self.y_train = np.array_split(data[\"Y_train\"], intervals)\n",
        "\n",
        "        self.num_batches_in_training = len(self.X_train)\n",
        "\n",
        "        self.X_val = data[\"X_val\"]\n",
        "        self.y_val = data[\"Y_val\"]\n",
        "\n",
        "        self.update_rule = sgd\n",
        "        self.loss_history = []\n",
        "        self.validation_history = []\n",
        "\n",
        "        self.iteration_num = 0\n",
        "\n",
        "\n",
        "    def _step(self, batch_id):\n",
        "        \"\"\"\n",
        "        Make a single gradient update. This is called by train() and should not\n",
        "        be called manually.\n",
        "        \"\"\"\n",
        "        # Make a minibatch of training data\n",
        "        X_batch = self.X_train[batch_id]\n",
        "        y_batch = self.y_train[batch_id]\n",
        "\n",
        "        # Compute loss and gradient\n",
        "        score, cache = self.model.forward(X_batch)\n",
        "        loss, dL = self.model.loss(score, y_batch)\n",
        "        _, grads = self.model.backward(dL, cache)\n",
        "\n",
        "        self.loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in self.model.params.items():\n",
        "            dw = grads[p]\n",
        "            next_w = self.update_rule(w, dw, self.learning_rate)\n",
        "            self.model.params[p] = next_w\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Optimization to train the model\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for batch_id in range(self.num_batches_in_training):\n",
        "\n",
        "                self._step(batch_id)\n",
        "\n",
        "                self.iteration_num += 1\n",
        "\n",
        "                if (self.iteration_num % self.validation_frequency == 0):\n",
        "                    self.validate()\n",
        "\n",
        "        self.validate()\n",
        "\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"\n",
        "        Checks the validation error of the model at the time it is being called.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        N = self.y_val.shape[0]\n",
        "        predictions = self.model.predict(self.X_val)\n",
        "\n",
        "        accuracy = np.count_nonzero(predictions == self.y_val.astype(int))\n",
        "\n",
        "        print(f\"The validation accuracy at iteration {self.iteration_num}  is \\\n",
        "              {(float(accuracy)/N)*100}%\")"
      ],
      "metadata": {
        "id": "kQP6qra2bpOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement two different classifiers for the (MNIST) dataset.\n",
        "(A classifier is a kind of model.)\n",
        "The models will both be linear,\n",
        "but will use different loss functions.\n",
        "The first will use the multiclass SVM loss,\n",
        "and the second will use the softmax  loss.\n",
        "\n",
        "The classifiers will be implemented using a common base class\n",
        "that implements training and prediction methods shared by the linear classifiers.\n",
        "Each of the two linear classifiers will be then derive\n",
        "from the base class,\n",
        "but override the loss function."
      ],
      "metadata": {
        "id": "vZfssCcKavEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; In the following cell,\n",
        "complete the definition of the `LinearClassifier` class\n",
        "by implementing the `__init__()`, `forward()`, `backward()`, and `predict()` methods.\n",
        "\n",
        "- The `__init__()` method initializes the class. You must generate a random weight matrix of shape `(input_dim+1, num_classes)`  \n",
        "- The `forward()` method generates the scores for given an input sample, by applying a `linear_forward()` transformation on the inputs `x` and weights matrix `self.params['W1']`\n",
        "- The `backward()` method returns the gradients with respect to the inputs and weights, using the `linear_backward()` method.\n",
        "Make sure the key for the returned dictionary `weights_gradient` matches the `self.params` dictionary.\n",
        "- The `predict()` method returns the labels predicted from the scores returned using the `self.forward()` method."
      ],
      "metadata": {
        "id": "y4b6y_Oj1qr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearClassifier(object):\n",
        "    \"\"\"\n",
        "    The base class for the linear classifier.\n",
        "\n",
        "    Note that this class does not implement gradient descent; instead, it\n",
        "    will interact with a separate Solver object that is responsible for running\n",
        "    optimization.\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim=784,\n",
        "                 num_classes=10):\n",
        "        self.params = {}\n",
        "        self.input_features = input_dim\n",
        "        self.num_classes = 10\n",
        "\n",
        "        # PUT YOUR CODE BELOW:\n",
        "        # Initialize the weights of the linear classifier. Weights should be\n",
        "        # initialized from a Gaussian centered at 0.0 with standard deviation\n",
        "        # equal to 1e-3, and biases should be initialized to zero.\n",
        "        # Store in the self.W dictionary with key name 'W1'\n",
        "\n",
        "        # The lines below do not need to be changed in the method\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - x: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = x.shape\n",
        "        num_classes = self.num_classes\n",
        "        out = None\n",
        "        cache = None\n",
        "\n",
        "        # PUT YOUR CODE BELOW:\n",
        "        # Implement this method. Generate the scores in out and store the old\n",
        "        # values into the cache.\n",
        "\n",
        "        # The lines below do not need to be changed\n",
        "\n",
        "\n",
        "        return out, (cache, )\n",
        "\n",
        "    def backward(self, dout, cache):\n",
        "        weight_gradients = {}\n",
        "        dx = None\n",
        "\n",
        "        # PUT YOUR CODE BELOW:\n",
        "        # Implement this method. Generate the gradients with respect to x from\n",
        "        # cache and set it dx, the upstream error signal.\n",
        "        # Store the gradient with respect to the weights in the weights_gradients\n",
        "        # dictionary. Make sure the key matches the ket of the params dictionary\n",
        "\n",
        "        # The lines below do not need to be changed\n",
        "\n",
        "        return (dx, weight_gradients)\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - x: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(x.shape[0])\n",
        "\n",
        "        # PUT YOUR CODE BELOW:\n",
        "        # Implement this method. Store the predicted labels in y_pred.\n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - scores: A numpy array of shape (N, C) containing a minibatch of N\n",
        "          data points; each point has dimension C, where C is the number of classes.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to scores; an array of the same shape as scores\n",
        "        \"\"\"\n",
        "        # The lines below do not need to be changed\n",
        "        # Do not implement anything here. The subclasses will override this method\n",
        "        pass"
      ],
      "metadata": {
        "id": "oLgT_g66T71G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Support Vector Machine\n",
        "\n",
        "The `LinearSVM` class defines an SVM-based linear classifier. The classifier uses the hinge loss to optimize the model parameters.\n",
        "The multiclass hinge loss for an input sample $x_i$ (a vector) is given by:\n",
        "\n",
        "$$\n",
        "L_i = \\sum_{j \\neq y_i} \\text{max}(0, s_j-s_{y_i}+1)\n",
        "$$\n",
        "\n",
        "Where, $y_i$ is the label of the $i$-th sample.\n",
        "The label is the correct class label where $0 \\leq y_i \\lt C$,\n",
        "where C is the number of classes.\n",
        "The scalar $s_{y_i}$ is the $y_i$-th element of the score vector.\n",
        "\n",
        "The average loss over N samples is therefore:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{N}\\sum^{N}_{i=1}L_i\n",
        "$$\n",
        "\n",
        "The per-sample gradient of the loss w.r.t. the score $s_j$ is given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial  L_i}{\\partial s_j} = \\left \\{\n",
        "\\begin{array}{ll}\n",
        "0 & s_j-s_{y_i}+1 \\leq 0    \\\\\n",
        "1 & j \\neq s_{y_i} \\text{ and } s_j-s_{y_i}+1 > 0 \\\\\n",
        "-\\sum_{i \\neq j}\\frac{\\partial L_i}{\\partial s_i} & j = s_{y_i}\n",
        "\\end{array}\n",
        "\\right..\n",
        "$$"
      ],
      "metadata": {
        "id": "rS6qcaqUcyqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the `svm_loss(scores, y_batch)` function in the following cell.\n",
        "Store the average loss the `loss` variable and the gradient w.r.t `scores` in the `dy` variable.\n",
        "This is the loss over multiple samples, therefore you should take the mean of the loss."
      ],
      "metadata": {
        "id": "fVJxsGOYTgxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss(scores, y_batch):\n",
        "    \"\"\"\n",
        "    Returns hinge loss of the scores and y_batch.\n",
        "\n",
        "    Inputs:\n",
        "    - scores: A numpy array of shape (N, C) containing a minibatch of N\n",
        "      data points; each point has dimension C, where C is the number of classes.\n",
        "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "    - reg: (float) regularization strength.\n",
        "\n",
        "    Returns: A tuple containing:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to scores; an array of the same shape as scores\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "    dy = np.zeros(scores.shape)\n",
        "    # PUT YOUR CODE BELOW:\n",
        "    # Implement the structured SVM loss, storing the\n",
        "    # result in loss. Make sure to take the mean of the loss.\n",
        "    # Hint: The intermediate results maybe useful for the gradient calculation\n",
        "\n",
        "\n",
        "    # PUT YOUR CODE BELOW:\n",
        "    # Implement the gradient for the SVM loss, storing the result\n",
        "    # in dy.\n",
        "    #\n",
        "    # Hint: Instead of computing the gradient from scratch, it may be easier\n",
        "    # to reuse some of the intermediate values that you used to compute the\n",
        "    # loss.\n",
        "\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    return loss, dy"
      ],
      "metadata": {
        "id": "S-_mC08N89UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660; Run the following cell to define the `LinearSVM` class with your implementation of the `svm_loss`"
      ],
      "metadata": {
        "id": "MMaObHBhamT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearSVM(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        return svm_loss(scores, y_batch)"
      ],
      "metadata": {
        "id": "VD43ltPTcw-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1.1 SVM Experiments\n",
        "\n",
        "In the next few cells run the `Solver` with SVM models on the training and validation data you've defined previously. Use the `DATA` dictionary you defined previously as the data parameter.\n",
        "\n",
        "As you have seen with previous assignments, optimizations can be highly dependent on the hyperparameters of the model. You should try multiple models with different learning rates. You may also increase the amount of time you train by increasing the number of epochs.\n",
        "\n",
        "Keep the top 5 best performing models and the worst performing model on the validation set."
      ],
      "metadata": {
        "id": "DNPZ7RoU2Oyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement hyperparameter validation loop in the next cell to train multiple models with different hyperparameters.\n",
        "Keep the top 5 best performing models on the validation set.\n",
        "You can try different learning rates.\n",
        "You may change the num_epochs, but be wary of timeouts."
      ],
      "metadata": {
        "id": "DD7et1xEF9h6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "US364cOjGC5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the testing performance of your top 5 performing models on the test set and print the results."
      ],
      "metadata": {
        "id": "Dl4R1tyz4VpS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "71qBuIr75ArL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the next cell to visualize the weights corresponding to each sample in the *best* performing SVM models. You should have ten 28x28 images.\n",
        "\n",
        "Make sure to rescale the  weights to be between 0 and 255.\n",
        "\n",
        "Depending on your learning rate and weights, it might not look so great. If all the images look the same but you have good accuracy, try subtracting the average of weights from weights of each class.\n",
        "\n",
        "You can add additional cells below."
      ],
      "metadata": {
        "id": "srNp8XPEbVAt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVDxnLZCbVQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Cross-Entropy Loss\n",
        "\n",
        "The `CrossEntropy` class defines the cross-entropy loss for training and prediction methods like the previous the linear classifiers.\n",
        "Because the cross-entropy is defined on probability distributions,\n",
        "the softmax function is usually applied to the output of a linear\n",
        "classifier to transform the raw scores into values that can be interpreted as probabilities.\n",
        "(Though, be cautious about actually using them as such.)\n",
        "However,\n",
        "we commonly refer to it just as the cross-entropy loss,\n",
        "with the implicit understanding that for deep learning,\n",
        "the cross-entropy is not computed on the raw scores,\n",
        "but rather the softmax of the raw scores.\n",
        "\n",
        "For a score vector $s$, the softmax activation of the $j$-th element is given by,\n",
        "\n",
        "$$\n",
        "\\sigma_j = \\frac{e^{s_{j}}}{\\sum^{M}_{k=1}e^{s_k}}\n",
        "$$\n",
        "\n",
        "A simple implementation of the softmax function can result in overflow.\n",
        "See\n",
        "[here](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/#:~:text=Computing%20softmax%20and%20numerical%20stability)\n",
        "for how to avoid this problem.\n",
        "\n",
        "The cross-entropy is a measure of the difference between two probability distributions.\n",
        "In the general case,\n",
        "the cross-entropy $H$ between the true probability distribution $P$ and the estimated probability distribution $Q$ is given by:\n",
        "\n",
        "$$\n",
        "H(P, Q)=-\\sum_{x \\in \\mathcal{X}} P(x) \\log Q(x)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{X}$ is the event space.\n",
        "It is a measure of how \"far off\" our estimated distribution $Q$ is from $P$.\n",
        "(Note that because $P$ and $Q$ are actually functions,\n",
        "$H$ in this case is a function operating on functions, also known as an *operator*.)\n",
        "\n",
        "In our case,\n",
        "$P$ is zero except for the correct label,\n",
        "and thus the cross-entropy reduces to simply the negative logarithm of the score corresponding to the correct class,\n",
        "which is just\n",
        "\n",
        "$$\n",
        "L_i = -\\log(\\sigma_{y_i}))\n",
        "$$\n",
        "\n",
        "where $y_i$ is the correct label of the $x_i$ input sample,\n",
        "and $\\sigma_{y_i}$ is the softmax output of the corresponding correct label. $L$ is then just the average over the $L_i$.\n",
        "\n",
        "The derivative of the softmax is given by\n",
        "\n",
        "$$\n",
        "\\frac{\\partial\\sigma_i}{\\partial s_j} = \\left \\{\n",
        "\\begin{array}{ll}\n",
        "\\sigma_i(1 - \\sigma_{j}) & i = j     \\\\\n",
        "-\\sigma_i\\sigma_j & i \\neq j  \\\\\n",
        "\\end{array}\n",
        "\\right. .\n",
        "$$\n",
        "\n",
        "Details on the derivation can be found [here](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/).\n",
        "\n",
        "The derivative of the negative logarithm is given by\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (-\\log)}{\\partial \\sigma_{y_i}} = -\\frac{1}{\\sigma_{y_i}}.\n",
        "$$"
      ],
      "metadata": {
        "id": "nhA92akOczjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the `cross_entropy_loss(scores, y_batch)` function in the following cell,\n",
        "consisting of the softmax followed by cross-entropy,\n",
        "as explained above.\n",
        "The function returns a tuple of `(loss, dy)` where\n",
        "`loss` is the cross-entropy loss based on the inputs\n",
        "and `dy` is the gradient of the loss with respect to the `scores` input.\n",
        "This is the loss over multiple samples,\n",
        "therefore you should take the mean of the loss."
      ],
      "metadata": {
        "id": "XJDqbXH4yxZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(scores, y_batch):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy layer\n",
        "\n",
        "    Inputs:\n",
        "    - scores: A numpy array containing the scores, of shape (N, C)\n",
        "    - y_batch: A numpy array containing the labels, of shape (N, 1)\n",
        "\n",
        "    Returns: A tuple containing:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to scores; an array of the same shape as scores\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0\n",
        "    dy = np.zeros(scores.shape)\n",
        "\n",
        "    # PUT YOUR CODE BELOW:\n",
        "    # Implement the cross-entropy loss, storing the\n",
        "    # result in loss. Make sure to take the mean of the loss.\n",
        "    # Hint: The intermediate results maybe useful for the gradient calculation\n",
        "\n",
        "\n",
        "\n",
        "    # PUT YOUR CODE BELOW:\n",
        "    # Implement the gradient for the cross-entropy loss, storing the result\n",
        "    # in dy.\n",
        "    #\n",
        "    # Hint: Instead of computing the gradient from scratch, it may be easier\n",
        "    # to reuse some of the intermediate values that you used to compute the\n",
        "    # loss.\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    return loss, dy"
      ],
      "metadata": {
        "id": "5hIEv5fl9llO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9660;\n",
        "Run the following cell to define the `CrossEntropy` classifier class."
      ],
      "metadata": {
        "id": "8h0EQF-yuu8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        return cross_entropy_loss(scores, y_batch)"
      ],
      "metadata": {
        "id": "RKYgCIcHczy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2.1 Cross-Entropy Experiments\n",
        "\n",
        "In the next few cells run the `Solver` with softmax models on the training and validation data you've defined previously,\n",
        "similarly to the SVM experiments.\n",
        "Use the `DATA` dictionary you defined previously as the data parameter.\n",
        "\n",
        "Keep the top 5 best performing models and the worst performing model on the validation set."
      ],
      "metadata": {
        "id": "Ug02WvIg5DT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the hyperparameter validation loop in the next cell to train multiple models with different hyperparameters.\n",
        "Keep the top 5 best performing models on the validation set.\n",
        "You can try different learning rates.\n",
        "You may change the number of epochs, but be wary of timeouts."
      ],
      "metadata": {
        "id": "qRFvUnysFwvt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05Od5fTg5Dqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the next cell to visualize the weights corresponding to each sample in the *best* performing softmax models.\n",
        "You should have ten 28x28 images.\n",
        "\n",
        "You can add additional cells below.\n",
        "\n",
        "Depending on your learning rate and weights, it might not look so great. If all the images look the same but you have good accuracy, try subtracting the average of weights from weights of each class."
      ],
      "metadata": {
        "id": "s8j9KoxfXgY5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SEDP9m-EZNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the testing performance of your top 5 performing softmax models on the test set and print the results."
      ],
      "metadata": {
        "id": "4GdpbgYg5D8W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxuJHeU45EVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Deeper Neural Networks (Very Slightly)"
      ],
      "metadata": {
        "id": "A7mX-suZStG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up until now, we have been working with linear classification models.\n",
        "Linear classification models are very adept at modelling data that have nice linear boundaries.\n",
        "In practice, real world data is rarely linear.\n",
        "Multilayer, fully-connected neural networks with non-linear activation functions on the other hand can model non-linear data-label relationships.\n",
        "Such models are a powerful extension to linear models and are the building blocks of modern deep learning.\n",
        "\n",
        "In this section,\n",
        "you will be implementing a two-layer, fully-connected neural network.\n",
        "You will also implement your own version of the rectified linear unit fuction (commonly reffered to as ReLU),\n",
        "a non-linear activation function."
      ],
      "metadata": {
        "id": "j60wKxwhXu2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 ReLU Function\n",
        "\n",
        "The ReLU function is given by:\n",
        "\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$"
      ],
      "metadata": {
        "id": "gP_YgE1JWj1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the following cell to complete the definition of the `ReLU_forward` function."
      ],
      "metadata": {
        "id": "SF20M-YOWSdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a ReLU actiivation.\n",
        "\n",
        "    The input x has shape (N, D) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (D). We will\n",
        "    transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, D)\n",
        "\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, D)\n",
        "    - cache: (x)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the ReLU forward pass. Store the result in\n",
        "    # out. You will need to reshape the input into rows.\n",
        "\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "    cache = (x,)\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "tQ-CcTqJStgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU derivative is given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f(x)}{\\partial x} = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "      0 & x \\leq 0 \\\\\n",
        "      1 & x > 0 \\\\\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "Where $f(x)$ is the ReLU function."
      ],
      "metadata": {
        "id": "g3LU7V4Ib4xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the following cell to complete the definition of the `ReLU_backward(d_upstream, cache)` function."
      ],
      "metadata": {
        "id": "ScME_zhFWXsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU_backward(d_upstream, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an linear layer.\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, D)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, D)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, D)\n",
        "    \"\"\"\n",
        "    x,  = cache\n",
        "    dx = None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the ReLU backward pass.\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "    return (dx, )"
      ],
      "metadata": {
        "id": "fTn6t0FaVBSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Two-layer Neural Network\n",
        "\n",
        "We will now implement the model for the two-layer NN.\n",
        "\n",
        "&#9658; Implement the definition of the two-layer neural network below.\n",
        "\n",
        "Similar to the `LinearClassifier` class, you should write the `__init__`, `forward`, `backward`, and `predict` methods. We will be using the cross-entropy loss for this network.\n",
        "\n",
        "Complete the following:\n",
        "- The `__init__()` method initializes the class. You must generate two random weight matrices. We will be using the bias trick, so the bias should concatenated to the weight matrix. They are initialized differently.\n",
        "\n",
        "- The `forward()` method generates the scores for given an input sample, by applying a `linear_forward()` and `ReLU_forward()` with appropriate inputs. Make sure to store and return the cache for the intermediate steps.\n",
        "\n",
        "- The `backward()` method returns the gradients with respect to the inputs and weights, using the `linear_backward()` and `ReLU_backward()`. Make sure the keys for the returned dictionary `weights_gradient` matches the keys in the `self.params` dictionary.\n",
        "\n",
        "- The `predict()` method returns the labels predicted from the scores returned using the `self.forward()` method.\n"
      ],
      "metadata": {
        "id": "jYDMTehxYP75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecure should be transform - relu - transform - softmax.\n",
        "\n",
        "    Note that this class does not implement gradient descent; instead, it\n",
        "    will interact with a separate Solver object that is responsible for running\n",
        "    optimization.\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 hidden_dim=100,\n",
        "                 num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        self.params = {}\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # PUT YOUR CODE BELOW: Initialize the weights of the two-layer net. Weights should be\n",
        "        # initialized from a Gaussian centered at 0.0 with standard deviation\n",
        "        # equal to weight_scale, and biases should be initialized to zero.\n",
        "        # All weights should be stored in the dictionary self.params, with first\n",
        "        # layer weights and using the keys 'W1' and second layer weights and using\n",
        "        # the keys 'W2'. Make sure to concatenate the weights and biases to make a\n",
        "        # a single matrix for the bias trick!\n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Implement the forward pass of the neural network and return the scores\n",
        "\n",
        "        Inputs:\n",
        "        - x: A numpy array containing input data, of shape (N, self.input_dim)\n",
        "\n",
        "\n",
        "        Returns a tuple of:\n",
        "        - out: output, of shape (N, D)\n",
        "        - Tuple of tuples:\n",
        "          - cache_lin_1: A tuple (x, w1)\n",
        "            - x: data, of shape (N, self.input_dim)\n",
        "            - w1: Weight of linear layer 1 of shape (self.input_dim+1, self.hidden_dim)\n",
        "          - cache_relu_1: A tuple (h, )\n",
        "            - h : data, of shape (N, self.hidden_dim)\n",
        "          - cache_lin_2:  A tuple (h, w2)\n",
        "            - h: data, of shape (N, self.hidden_dim)\n",
        "            - w2: weight of linear 2 of shape (self.hidden_dim+1, C)\n",
        "        \"\"\"\n",
        "        out = None\n",
        "        N, feature_dim = x.shape\n",
        "        cache_lin_1, cache_relu_1, cache_lin_2 = None, None, None\n",
        "\n",
        "        if (feature_dim != self.input_dim):\n",
        "            raise Exception(f\"The input feature dimension of {feature_dim} does \\\n",
        "                            not match the expected feature dimension of \\\n",
        "                            {self.input_dim} \")\n",
        "\n",
        "\n",
        "        # PUT YOUR CODE BELOW: Perform a forward pass of the two-layer net.\n",
        "        # The architecture is transform - relu - transform\n",
        "        # Make to store the appropriate cache in the appropriate variables\n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "\n",
        "        return out, (cache_lin_1, cache_relu_1, cache_lin_2)\n",
        "\n",
        "\n",
        "    def backward(self, dout, cache):\n",
        "        \"\"\"\n",
        "        Implement the backward pass of the neural network and return the\n",
        "        gradients w.r.t the input, and the weights\n",
        "\n",
        "        Inputs:\n",
        "        - dout: Upstream derivative, of shape (N, C)\n",
        "        - cache: Tuple of tuples:\n",
        "          - cache_lin_1: A tuple (x, w1)\n",
        "            - x: data, of shape (N, self.input_dim)\n",
        "            - w1: Weight of linear layer 1 of shape (self.input_dim+1, self.hidden_dim)\n",
        "          - cache_relu_1: A tuple (h, )\n",
        "            - h : data, of shape (N, self.hidden_dim)\n",
        "          - cache_lin_2:  A tuple (h, w2)\n",
        "            - h: data, of shape (N, self.hidden_dim)\n",
        "            - w2: weight of linear 2 of shape (self.hidden_dim+1, C)\n",
        "\n",
        "        Returns a tuple of:\n",
        "          - dx: A numpy array of the gradient with respect to x, of shape (N, D)\n",
        "          - weight_gradients: A dictionary of numpy arrays containing the\n",
        "              gradients with respect to the weights.\n",
        "        \"\"\"\n",
        "\n",
        "        weight_gradients = {}\n",
        "        dx = None\n",
        "\n",
        "        N, classes = dout.shape\n",
        "\n",
        "        cache_lin_1, cache_relu_1, cache_lin_2 = cache\n",
        "\n",
        "        if (classes != self.num_classes):\n",
        "            raise Exception(f\"The output class dimension of {classes} does \\\n",
        "                            not match the expected number of classes \\\n",
        "                            {self.num_classes} \")\n",
        "\n",
        "        # PUT YOUR CODE BELOW: Perform a backward pass of the two-layer net.\n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "        return (dx, weight_gradients)\n",
        "\n",
        "    def predict(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        Implement the predictions from the forward pass of the neural network and\n",
        "        returns it.\n",
        "\n",
        "        Inputs:\n",
        "        - x: Input data, of shape (N, self.input_dim)\n",
        "\n",
        "        Returns a tuple of:\n",
        "          - predictions: A numpy array of shape (N, ) of the predicted class per sample\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        y_pred = None\n",
        "\n",
        "\n",
        "        # PUT YOUR CODE BELOW: Predict the classes of using the two-layer net.\n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss using the cross_entropy_loss function and its\n",
        "        derivative: -1/np.sqrt(input_dim).\n",
        "        Inputs:\n",
        "        - scores: A numpy array of shape (N, C) containing a minibatch of N\n",
        "          data points; each point has dimension C, where C is the number of classes.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to scores; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        # The lines below do not need to be changed in this method.\n",
        "        return cross_entropy_loss(scores, y_batch)"
      ],
      "metadata": {
        "id": "dSr_3dSiYQOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Experiments\n",
        "\n",
        "Similar to the linear classifiers, you also want to identify the best configuration of hyperparameters that perform the best for your dataset. Similar to the case of the linear models, you can vary the learning rate for your solver. You should use the `Solver` class for these models as well. Use the `DATA` dictionary you defined previously as the data parameter.\n",
        "\n",
        "Additionaly, the neural network provides another hyperparameter to vary, the the number of neurons in the hidden layer.\n",
        "\n",
        "Adding a large of number of neurons may cause a large degradation in performance, the linear transformation scales as $O(N^3)$ with the number of neurons."
      ],
      "metadata": {
        "id": "yovrIeNablvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement a hyperparameter validation loop in the next cell to train multiple models with different hyperparameters. Keep the top-5 best performing models on the validation set. You may change learning rate, and hidden dims. You may change the num_epochs, but be wary of timeouts."
      ],
      "metadata": {
        "id": "N8zQgNiK-I9g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwZysCge-Js5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement the testing performance of your top-5 performing NN models on the test set and print the results. You can add additional cells below."
      ],
      "metadata": {
        "id": "zi1hTKxU-Kap"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QL6IbrNo-PTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "&#9658; Implement visualization for the `W1` weights of the *best* performing NN models. There are `hidden_dim` many of them per model. You should visualize a subset of the weights. You can select the columns at random.\n",
        "\n",
        "You can add additional cells below."
      ],
      "metadata": {
        "id": "qC8IL-RNdCOY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-CGHrOtdCb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}